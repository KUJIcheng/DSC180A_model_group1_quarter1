{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLama2 Flops Calcuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class theoretic_llama_model_flops_calculator():\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the FLOPs calculator with the model and input shape.\n",
    "        :param model: The neural network model (could be a PyTorch or TensorFlow model).\n",
    "        :param input_shape: The shape of the input tensor for FLOP calculations.\n",
    "        \"\"\"\n",
    "        self.s = kwargs['max_sequence_length']\n",
    "        self.h = kwargs['hidden_dimension']\n",
    "        self.I = kwargs['intermediate_dimension']\n",
    "        self.l = kwargs['number_of_layers']\n",
    "        self.v = kwargs['vocabulary_dimension']\n",
    "        self.b = kwargs['batch_size']\n",
    "        self.n = kwargs['number_of_heads']\n",
    "        self.d = kwargs['head_dimension']\n",
    "        self.float_type = kwargs['float_type']\n",
    "        self.mode = kwargs['mode']\n",
    "\n",
    "    def calculate_embedding_forward(self):\n",
    "        flops = 2 * self.b * self.s * self.h * self.v\n",
    "        memory_IO = self.b * self.s * self.v + self.h * self.v + self.b * self.s * self.h\n",
    "        byte_num = np.dtype(self.float_type).itemsize\n",
    "        memory_IO = memory_IO * byte_num\n",
    "        return flops, memory_IO\n",
    "    \n",
    "    def calculate_normalization_forward(self):\n",
    "        flops = self.b * (4 * self.s * self.h  + 2 * self.s )\n",
    "        memory_IO = self.b * self.s * self.h + self.h\n",
    "        byte_num = np.dtype(self.float_type).itemsize\n",
    "        memory_IO = memory_IO * byte_num\n",
    "        return flops, memory_IO\n",
    "    \n",
    "    def calculate_attention_forward(self):\n",
    "        if self.mode == 'train':\n",
    "            q_k_v_flops = 3 * 2 * self.b * self.s * self.n * self.d * self.h\n",
    "            q_k_v_memory_IO = 3 * 2 * self.b * self.s * self.n * self.d + 3 * self.h**2\n",
    "            soft_q_k_flops = 2 * self.b * self.s**2 * self.n * self.d + 3 * self.b * self.s**2 * self.n\n",
    "            soft_q_k_memory_IO = 2 * self.b * self.s * self.n * self.d + self.b * self.s**2 * self.n\n",
    "            v_flops = 2 * self.b * self.n *self.s**2 * self.d\n",
    "            v_memory_IO = self.b * self.s**2 * self.n + self.b * self.n *self.s * self.d\n",
    "            o_concat_flops =  2 * self.b * self.s * self.h**2\n",
    "            o_concat_memory_IO = self.b * self.s * self.h + self.h**2\n",
    "            flops = q_k_v_flops + soft_q_k_flops + v_flops + o_concat_flops\n",
    "            memory_IO = q_k_v_memory_IO + soft_q_k_memory_IO + v_memory_IO + o_concat_memory_IO\n",
    "        elif self.mode == 'inference':\n",
    "            q_k_v_flops = 3 * 2 * self.b  * self.n * self.d * self.h\n",
    "            q_k_v_memory_IO = 3 * 2 * self.b * self.n * self.d + 3 * self.h**2\n",
    "            soft_q_k_flops = 2 * self.b * (self.s + 1) * self.n * self.d + 3 * self.b * (self.s + 1)  * self.n\n",
    "            soft_q_k_memory_IO = self.b * self.n * self.d + self.b * (self.s + 1) * self.n * self.d + self.b * (self.s + 1) * self.n\n",
    "            v_flops = 2 * self.b * self.n * (self.s + 1) * self.d\n",
    "            v_memory_IO = self.b * (self.s + 1) * self.n + self.b * self.n * self.d\n",
    "            o_concat_flops =  2 * self.b * self.h**2\n",
    "            o_concat_memory_IO = self.b * self.h + self.h**2\n",
    "            flops = q_k_v_flops + soft_q_k_flops + v_flops + o_concat_flops\n",
    "            memory_IO = q_k_v_memory_IO + soft_q_k_memory_IO + v_memory_IO + o_concat_memory_IO\n",
    "            \n",
    "\n",
    "        return flops, memory_IO\n",
    "    \n",
    "    def calculate_mlp_forward(self):\n",
    "        # activation function as one operator + sum up\n",
    "        gate_up_flops= 2 * 2 * self.b * self.s * self.h * self.I + 2 * self.b * self.h *self.I\n",
    "        gate_up_memory_IO = 2 * (self.b * self.s * self.h + self.h * self.I + self.b * self.s *self.I)\n",
    "        down_flops = 2 * self.b * self.s * self.h * self.I\n",
    "        down_memory_IO = self.b * self.s * self.I + self.h * self.I + self.b * self.s * self.h\n",
    "        flops = gate_up_flops + down_flops\n",
    "        memory_IO = gate_up_memory_IO + down_memory_IO\n",
    "        return flops, memory_IO\n",
    "\n",
    "    def calculate_final_linear_forward(self):\n",
    "        linear_flops = 2 * self.b * self.s * self.h * self.v\n",
    "        linear_memory_IO = self.b * self.s * self.h + self.h * self.v + self.b * self.s * self.v\n",
    "\n",
    "        return linear_flops, linear_memory_IO\n",
    "    \n",
    "    def calculate_residual_forward(self):\n",
    "        residual_flops = 2 * self.b * self.s * self.h \n",
    "        residual_memory_IO = 2 * self.b * self.s * self.h \n",
    "        return residual_flops, residual_memory_IO\n",
    "    \n",
    "    def calculate_flops_forward(self):\n",
    "        emb_flops, _ = self.calculate_embedding_forward()\n",
    "        atten_flops, _ = self.calculate_attention_forward()\n",
    "        norm_flops, _ = self.calculate_normalization_forward()\n",
    "        mlp_flops, _ = self.calculate_mlp_forward()\n",
    "        final_linear_flops, _ = self.calculate_final_linear_forward()\n",
    "        residual_flops, _ = self.calculate_residual_forward()\n",
    "        all_flops = emb_flops + self.l * (2 * norm_flops + 2 * residual_flops + atten_flops + mlp_flops) + norm_flops + final_linear_flops\n",
    "        flops_prop = {\n",
    "            \"Embedding Layer\": emb_flops / all_flops,\n",
    "            \"Normalization\": (self.l * 2 * norm_flops + norm_flops) / all_flops,\n",
    "            \"Residual\": self.l * 2 * residual_flops / all_flops,\n",
    "            \"Attention\": self.l * atten_flops / all_flops,\n",
    "            \"MLP\":  self.l * mlp_flops / all_flops,\n",
    "            \"Linear\": final_linear_flops / all_flops\n",
    "        }\n",
    "\n",
    "        return all_flops, flops_prop\n",
    "    \n",
    "    def calculate_flops_backward(self):\n",
    "        forward_flops, _ = self.calculate_flops_forward()\n",
    "        return 2 * forward_flops\n",
    "\n",
    "    def calculate_flops(self):\n",
    "        forward_flops, forward_flop_prop = self.calculate_flops_forward()\n",
    "        return forward_flops + self.calculate_flops_backward(), forward_flop_prop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Train TeraFLOPs for LLaMA-2 7B: 192.167844274176; Flop Proportion: {'Embedding Layer': 0.016762562353585586, 'Normalization': 6.81062222945422e-05, 'Residual': 3.352512470717117e-05, 'Attention': 0.41276133539469145, 'MLP': 0.5536119085511356, 'Linear': 0.016762562353585586}\n"
     ]
    }
   ],
   "source": [
    "llama2_7b_config = {\n",
    "    'max_sequence_length': 4096,\n",
    "    'hidden_dimension': 4096,\n",
    "    'intermediate_dimension': 11008,\n",
    "    'number_of_layers': 32,\n",
    "    'vocabulary_dimension': 32000,\n",
    "    'batch_size': 1,  # Example batch size\n",
    "    'number_of_heads': 32,\n",
    "    'head_dimension': 128,\n",
    "    'float_type': 'float32',\n",
    "    'mode': 'train'\n",
    "}\n",
    "\n",
    "llama_flops_calculator = theoretic_llama_model_flops_calculator(**llama2_7b_config)\n",
    "total_flops, flops_prop = llama_flops_calculator.calculate_flops()\n",
    "print(f\"Total Train TeraFLOPs for LLaMA-2 7B: {total_flops / 1e12}; Flop Proportion: {flops_prop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Inference GFLOPs for LLaMA-2 7B: 49.093872006; Flop Proportion: {'Embedding Layer': 0.016018944276871994, 'Normalization': 6.508490508977354e-05, 'Residual': 3.2037888553743995e-05, 'Attention': 0.26251883425338474, 'MLP': 0.7053461543992278, 'Linear': 0.016018944276871994}\n"
     ]
    }
   ],
   "source": [
    "llama2_7b_config = {\n",
    "    'max_sequence_length': 1,\n",
    "    'hidden_dimension': 4096,\n",
    "    'intermediate_dimension': 11008,\n",
    "    'number_of_layers': 32,\n",
    "    'vocabulary_dimension': 32000,\n",
    "    'batch_size': 1,  # Example batch size\n",
    "    'number_of_heads': 32,\n",
    "    'head_dimension': 128,\n",
    "    'float_type': 'float32',\n",
    "    'mode': 'inference'\n",
    "}\n",
    "\n",
    "llama_flops_calculator = theoretic_llama_model_flops_calculator(**llama2_7b_config)\n",
    "total_flops, flops_prop = llama_flops_calculator.calculate_flops()\n",
    "print(f\"Total Inference GFLOPs for LLaMA-2 7B: {total_flops / 1e9}; Flop Proportion: {flops_prop}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
