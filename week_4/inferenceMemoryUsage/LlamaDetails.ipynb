{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59e96310-f777-401e-94a2-be683a40a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers\n",
    "# pip install thop\n",
    "# pip install torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae313adc-d0e7-418a-b6c4-c6aa28c47bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494b8f902b774b108f8e5b491df9ee43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 定义模型和 tokenizer 的路径\n",
    "model_directory = r\"Z:/llmfile/Meta-Llama-3.1-8B-Instruct/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/5206a32e0bd3067aef1ce90f5528ade7d866253f\"\n",
    "# model_directory = r\"Z:/llmfile/Llama-2-7B-hf/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9\"\n",
    "\n",
    "# 加载模型和 tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_directory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "\n",
    "# 打印模型结构\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af4cd26-f164-4fd9-aa2f-69fdfc2ab0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 8030261248\n",
      "Trainable parameters: 8030261248\n"
     ]
    }
   ],
   "source": [
    "# 计算模型参数数量\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efcddd3c-9b29-4bbb-938f-4b21ccf43e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "FLOPs: 13531294466048.0\n",
      "Parameters: 6607077376.0\n"
     ]
    }
   ],
   "source": [
    "from thop import profile\n",
    "import torch\n",
    "\n",
    "# 定义输入的形状（虚拟输入，确保类型为 LongTensor）\n",
    "input_shape = (1, 512)  # batch size 1, sequence length 2048\n",
    "long_input = torch.ones(input_shape, dtype=torch.long)  # 确保输入为 LongTensor\n",
    "\n",
    "# 使用 thop 来计算 FLOPs 和参数数量\n",
    "macs, params = profile(model, inputs=(long_input,))\n",
    "\n",
    "# 打印 FLOPs 和参数信息\n",
    "print(f\"FLOPs: {macs}\")\n",
    "print(f\"Parameters: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2c1aefd-0e94-4ce5-bad5-a9ca077df862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2349de1e9f74603b2aa8bbe27207252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "Inference FLOPs (per forward pass): 3382.823616512 GFLOPs\n",
      "Training FLOPs (per batch): 10.148470849536 TFLOPs\n",
      "Total Training FLOPs: 324.751067185152 TFLOPs\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "model_directory = r\"Z:/llmfile/Llama-2-7B-hf/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_directory)\n",
    "\n",
    "# 定义输入张量\n",
    "input_shape = (1, 512)  # batch size 1, sequence length 2048\n",
    "dummy_input = torch.ones(input_shape, dtype=torch.long)\n",
    "\n",
    "# 前向传播 FLOPs 计算\n",
    "macs, params = profile(model, inputs=(dummy_input,))\n",
    "print(f\"Inference FLOPs (per forward pass): {macs / 1e9} GFLOPs\")\n",
    "\n",
    "# 假设反向传播 FLOPs 是前向传播的两倍\n",
    "training_flops_per_batch = 3 * macs  # 1 次前向传播 + 2 次反向传播\n",
    "print(f\"Training FLOPs (per batch): {training_flops_per_batch / 1e12} TFLOPs\")\n",
    "\n",
    "# 假设有 n_batches 个 batch 训练\n",
    "n_batches = 32  # 例如 10000 个 batch\n",
    "total_training_flops = training_flops_per_batch * n_batches\n",
    "print(f\"Total Training FLOPs: {total_training_flops / 1e12} TFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31195f21-540c-4745-a8f1-a11df5c65da5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
